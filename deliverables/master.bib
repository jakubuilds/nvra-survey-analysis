
@article{logan_segregation_2017,
	title = {Segregation and {Homeownership} in the {Early} {Twentieth} {Century}},
	volume = {107},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171081},
	doi = {10.1257/aer.p20171081},
	abstract = {We use new county-level segregation estimates for the period of 1880 to 1940 to document a general rise in residential segregation in both urban and rural counties occurring alongside rising homeownership rates. However, we find a negative correlation between segregation and homeownership across space for both black and white households. Following Fetter (2013), we show that living in a more segregated county substantially reduced the impact of GI Bill benefits on white homeownership rates, suggesting that segregated locations potentially hindered both white and black homeownership.},
	language = {en},
	number = {5},
	urldate = {2019-03-18},
	journal = {American Economic Review},
	author = {Logan, Trevon D. and Parman, John M.},
	month = may,
	year = {2017},
	keywords = {Canada: 1913-, Regional and Urban History: U.S., Canada: 1913-, Urban, Rural, Regional, Real Estate, and Transportation Economics: Regional Migration, Canada: Pre-1913, Economic History: Labor and Consumers, Demography, Education, Health, Welfare, Income, Wealth, Religion, and Philanthropy: U.S., Canada: Pre-1913, Regional and Urban History: U.S., Economics of Minorities, Races, Indigenous Peoples, and Immigrants, Neighborhood Characteristics, Housing Supply and Markets, Non-labor Discrimination, Economic History: Labor and Consumers, Demography, Education, Health, Welfare, Income, Wealth, Religion, and Philanthropy: U.S., Population, Regional Labor Markets},
	pages = {410--414}
}

@article{logan_validating_2016,
	title = {Validating {Population} {Estimates} for {Harmonized} {Census} {Tract} {Data}, 2000–2010},
	volume = {106},
	issn = {2469-4452},
	url = {https://doi.org/10.1080/24694452.2016.1187060},
	doi = {10.1080/24694452.2016.1187060},
	abstract = {Social scientists regularly rely on population estimates when studying change in small areas over time. Census tract data in the United States are a prime example, as there are substantial shifts in tract boundaries from decade to decade. This study compares alternative estimates of the 2000 population living within 2010 tract boundaries to the Census Bureau's own retabulation. All methods of estimation are subject to error; this is the first study to directly quantify the error in alternative interpolation methods for U.S. census tracts. A simple areal weighting method closely approximates the estimates provided by one standard source (the Neighborhood Change Data Base), with some improvement provided by considering only area not covered by water. More information is used by the Longitudinal Tract Data Base (LTDB), which relies on a combination of areal and population interpolation as well as ancillary data about water-covered areas. Another set of estimates provided by the National Historical Geographic Information Systems (NHGIS) uses data about land cover in 2001 and the current road network and distribution of population and housing units at the block level. Areal weighting alone results in a large error in a substantial share of tracts that were divided in complex ways. The LTDB and NHGIS perform much better in all situations but are subject to some error when boundaries of both tracts and their component blocks are redrawn. Users of harmonized tract data should be watchful for potential problems in either of these data sources.},
	number = {5},
	urldate = {2019-03-18},
	journal = {Annals of the American Association of Geographers},
	author = {Logan, John R. and Stults, Brian J. and Xu, Zengwang},
	month = sep,
	year = {2016},
	keywords = {boundaries, census data, census tracts, datos censales, interpolación, interpolation, límites, secciones censales, 人口普查单位, 人口普查数据, 内插法。, 边界},
	pages = {1013--1029}
}

@article{logan_interpolating_2014,
	title = {Interpolating {U}.{S}. {Decennial} {Census} {Tract} {Data} from as {Early} as 1970 to 2010: {A} {Longitudinal} {Tract} {Database}},
	volume = {66},
	issn = {0033-0124},
	shorttitle = {Interpolating {U}.{S}. {Decennial} {Census} {Tract} {Data} from as {Early} as 1970 to 2010},
	url = {https://doi.org/10.1080/00330124.2014.905156},
	doi = {10.1080/00330124.2014.905156},
	abstract = {Differences in the reporting units of data from diverse sources and changes in units over time are common obstacles to analysis of areal data. We compare common approaches to this problem in the context of changes over time in the boundaries of U.S. census tracts. In every decennial census, many tracts are split, consolidated, or changed in other ways from the previous boundaries to reflect population growth or decline. We examine two interpolation methods to create a bridge between years, one that relies only on areal weighting and another that also introduces population weights. Results demonstrate that these approaches produce substantially different estimates for variables that involve population counts, but they have a high degree of convergence for variables defined as rates or averages. Finally, the article describes the Longitudinal Tract Database (LTDB), through which we are making available public-use tools to implement these methods to create estimates within 2010 tract boundaries for any tract-level data (from the census or other sources) that are available for prior years as early as 1970.},
	number = {3},
	urldate = {2019-03-18},
	journal = {The Professional Geographer},
	author = {Logan, John R. and Xu, Zengwang and Stults, Brian J.},
	month = jul,
	year = {2014},
	pmid = {25140068},
	keywords = {2010 Census, 2010 年人口普查, Censo de 2010, areal interpolation, census geography, census tract, distrito censal, geografía censal, interpolación de población, interpolación espacial, population interpolation, 人口内插, 人口普查区, 人口普查地理, 面积内插},
	pages = {412--420}
}

@article{iacus_theory_2019,
	title = {A {Theory} of {Statistical} {Inference} for {Matching} {Methods} in {Causal} {Research}},
	volume = {27},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/theory-of-statistical-inference-for-matching-methods-in-causal-research/C047EB2F24096F5127E777BDD242AF46},
	doi = {10.1017/pan.2018.29},
	abstract = {Researchers who generate data often optimize efficiency and robustness by choosing stratified over simple random sampling designs. Yet, all theories of inference proposed to justify matching methods are based on simple random sampling. This is all the more troubling because, although these theories require exact matching, most matching applications resort to some form of ex post stratification (on a propensity score, distance metric, or the covariates) to find approximate matches, thus nullifying the statistical properties these theories are designed to ensure. Fortunately, the type of sampling used in a theory of inference is an axiom, rather than an assumption vulnerable to being proven wrong, and so we can replace simple with stratified sampling, so long as we can show, as we do here, that the implications of the theory are coherent and remain true. Properties of estimators based on this theory are much easier to understand and can be satisfied without the unattractive properties of existing theories, such as assumptions hidden in data analyses rather than stated up front, asymptotics, unfamiliar estimators, and complex variance calculations. Our theory of inference makes it possible for researchers to treat matching as a simple form of preprocessing to reduce model dependence, after which all the familiar inferential techniques and uncertainty calculations can be applied. This theory also allows binary, multicategory, and continuous treatment variables from the outset and straightforward extensions for imperfect treatment assignment and different versions of treatments.},
	language = {en},
	number = {1},
	urldate = {2019-03-15},
	journal = {Political Analysis},
	author = {Iacus, Stefano M. and King, Gary and Porro, Giuseppe},
	month = jan,
	year = {2019},
	keywords = {causal inference, matching, multiple treatments, observational studies, stratification},
	pages = {46--68}
}

@article{harden_replications_2019,
	title = {Replications in {Context}: {A} {Framework} for {Evaluating} {New} {Methods} in {Quantitative} {Political} {Science}},
	volume = {27},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Replications in {Context}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/replications-in-context-a-framework-for-evaluating-new-methods-in-quantitative-political-science/AFFB0A19C6AC13287FC04D0587897593},
	doi = {10.1017/pan.2018.54},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1047198718000542/resource/name/firstPage-S1047198718000542a.jpg},
	language = {en},
	number = {1},
	urldate = {2019-03-15},
	journal = {Political Analysis},
	author = {Harden, Jeffrey J. and Sokhey, Anand E. and Wilson, Hannah},
	month = jan,
	year = {2019},
	keywords = {quantitative methods, replication, researcher degrees of freedom},
	pages = {119--125}
}

@article{plumper_not_2019,
	title = {Not so {Harmless} {After} {All}: {The} {Fixed}-{Effects} {Model}},
	volume = {27},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Not so {Harmless} {After} {All}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/not-so-harmless-after-all-the-fixedeffects-model/5DD67D1D95F573B11EB189BA72338854/core-reader},
	doi = {10.1017/pan.2018.17},
	abstract = {The fixed-effects estimator is biased in the presence of dynamic misspecification and omitted within variation correlated with one of the regressors. We argue and demonstrate that fixed-effects estimates can amplify the bias from dynamic misspecification and that with omitted time-invariant variables and dynamic misspecifications, the fixed-effects estimator can be more biased than the ‘naïve’ OLS model. We also demonstrate that the Hausman test does not reliably identify the least biased estimator when time-invariant and time-varying omitted variables or dynamic misspecifications exist. Accordingly, empirical researchers are ill-advised to rely on the Hausman test for model selection or use the fixed-effects model as default unless they can convincingly justify the assumption of correctly specified dynamics. Our findings caution applied researchers to not overlook the potential drawbacks of relying on the fixed-effects estimator as a default. The results presented here also call upon methodologists to study the properties of estimators in the presence of multiple model misspecifications. Our results suggest that scholars ought to devote much more attention to modeling dynamics appropriately instead of relying on a default solution before they control for potentially omitted variables with constant effects using a fixed-effects specification.},
	language = {en},
	number = {1},
	urldate = {2019-03-15},
	journal = {Political Analysis},
	author = {Plümper, Thomas and Troeger, Vera E.},
	month = jan,
	year = {2019},
	keywords = {Monte Carlo simulation, consistency, efficiency, misspecification, omitted variable bias, panel data},
	pages = {21--45}
}

@article{steenbergen_modeling_2002,
	title = {Modeling {Multilevel} {Data} {Structures}},
	volume = {46},
	issn = {00925853, 15405907},
	url = {http://www.jstor.org/stable/3088424},
	doi = {10.2307/3088424},
	abstract = {Multilevel data are structures that consist of multiple units of analysis, one nested within the other. Such data are becoming quite common in political science and provide numerous opportunities for theory testing and development. Unfortunately, this type of data typically generates a number of statistical problems, of which clustering is particularly important. To exploit the opportunities offered by multilevel data, and to solve the statistical problems inherent in them, special statistical techniques are required. In this article, we focus on a technique that has become popular in educational statistics and sociology-multilevel analysis. In multilevel analysis, researchers build models that capture the layered structure of multilevel data, and determine how layers interact and impact a dependent variable of interest. Our objective in this article is to introduce the logic and statistical theory behind multilevel models, to illustrate how such models can be applied fruitfully in political science, and to call attention to some of the pitfalls in multilevel analysis.},
	number = {1},
	journal = {American Journal of Political Science},
	author = {Steenbergen, Marco R. and Jones, Bradford S.},
	year = {2002},
	pages = {218--237}
}

@article{noauthor_notitle_nodate
}

@book{gelman_data_2007,
	address = {New York},
	title = {Data {Analysis} {Using} {Regression} and {Multilevel}/{Hierarchical} {Models}},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	year = {2007}
}

@article{king_making_2000,
	title = {Making the {Most} of {Statistical} {Analyses}: {Improving} {Interpretation} and {Presentation}},
	volume = {44},
	number = {2},
	journal = {American Journal of Political Science},
	author = {King, Gary and Tomz, Michael and Wittenberg, Jason},
	year = {2000},
	pages = {341--355}
}

@book{virginia_department_of_behavioral_health_and_developmental_services_core_2014,
	title = {Core {Services} {Taxonomy} 7.3},
	url = {http://www.dbhds.virginia.gov/library/community contracting/occ-2010-coreservicestaxonomy7-2v2.pdf},
	urldate = {2017-10-26},
	author = {Virginia Department of Behavioral Health {and} Developmental Services},
	year = {2014}
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {0162-1459},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478354},
	doi = {10.1080/01621459.1986.10478354},
	abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
	number = {396},
	urldate = {2016-03-14},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	year = {1986},
	pages = {945--960}
}

@article{sekhon_opiates_2009,
	title = {Opiates for the {Matches}: {Matching} {Methods} for {Causal} {Inference}},
	volume = {12},
	shorttitle = {Opiates for the {Matches}},
	url = {http://dx.doi.org/10.1146/annurev.polisci.11.060606.135444},
	doi = {10.1146/annurev.polisci.11.060606.135444},
	abstract = {In recent years, there has been a burst of innovative work on methods for estimating causal effects using observational data. Much of this work has extended and brought a renewed focus on old approaches such as matching, which is the focus of this review. The new developments highlight an old tension in the social sciences: a focus on research design versus a focus on quantitative models. This realization, along with the renewed interest in field experiments, has marked the return of foundational questions as opposed to a fascination with the latest estimator. I use studies of get-out-the-vote interventions to exemplify this development. Without an experiment, natural experiment, a discontinuity, or some other strong design, no amount of econometric or statistical modeling can make the move from correlation to causation persuasive.},
	number = {1},
	urldate = {2016-03-14},
	journal = {Annual Review of Political Science},
	author = {Sekhon, Jasjeet S.},
	year = {2009},
	keywords = {Neyman-Rubin model, causal inference, matching},
	pages = {487--508}
}

@article{george_joining_2002,
	title = {Joining {Forces}: {The} {Role} of {Collaboration} in the {Development} of {Legal} {Thought}},
	volume = {52},
	issn = {0022-2208},
	shorttitle = {Joining {Forces}},
	url = {http://www.jstor.org/stable/42898305},
	number = {4},
	urldate = {2016-03-11},
	journal = {Journal of Legal Education},
	author = {George, Tracey E. and Guthrie, Chris},
	year = {2002},
	pages = {559--582}
}

@article{rachlinski_evidence-based_2010,
	title = {Evidence-{Based} {Law}},
	volume = {96},
	url = {http://heinonline.org/HOL/Page?handle=hein.journals/clqv96&id=911&div=34&collection=journals},
	number = {4},
	journal = {Cornell Law Review},
	author = {Rachlinski, Jeffrey J.},
	year = {2010},
	pages = {901--924}
}

@article{diamond_empirical_2010,
	title = {Empirical {Legal} {Scholarship} in {Law} {Reviews}},
	volume = {6},
	url = {http://dx.doi.org/10.1146/annurev-lawsocsci-102209-152848},
	doi = {10.1146/annurev-lawsocsci-102209-152848},
	abstract = {Despite persistent calls for more empirical legal scholarship, only recently have scholars provided evidence that empirical legal scholarship has indeed entered the mainstream of the legal academy. Defining empirical scholarship as the systematic organization of a series of observations with the method of data collection and analysis made available to the audience, we analyzed the content of 60 law review volumes published in the years 1998 and 2008. Our content analysis revealed that by 2008 nearly half of law review articles included some empirical content. Production of original research is less common. The highest-ranked law reviews published more articles and included more complex research designs. Analyzing the benefits and costs of publishing in law reviews, we predict that law reviews will see more original empirical scholarship in the future, despite the increased availability of peer-reviewed publication outlets.},
	number = {1},
	urldate = {2016-03-11},
	journal = {Annual Review of Law and Social Science},
	author = {Diamond, Shari Seidman and Mueller, Pam},
	year = {2010},
	keywords = {citation, content analysis, measurement},
	pages = {581--599}
}

@article{lopucki_dawn_2016,
	title = {Dawn of the {Discipline}-{Based} {Law} {Faculty}},
	volume = {65},
	number = {3},
	journal = {Journal of Legal Education},
	author = {LoPucki, Lynn M.},
	year = {2016},
	pages = {506--542}
}

@article{lopucki_disciplining_2015,
	title = {Disciplining {Legal} {Scholarship}},
	volume = {90},
	issn = {00413992},
	url = {http://www.proxy.its.virginia.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=111166254&site=eds-live},
	abstract = {US. law schools are hiring large proportions of J.D.-Ph.D.s in tenure-track faculty positions in an effort to increase the quantity and quality of empirical legal scholarship. That effort is failing. The new recruits bring methods and objectives unsuited to law. They produce lower-than-predicted levels of empiricism because they compete on the basis of methodological sophistication, devote time and resources to disputes over arcane issues in statistics and methodology, prefer to collaborate with other Ph.D.s, and intimidate empiricists whose work does not require high levels of methodological sophistication. In short, Ph.D.s impose the cultures of them disciplines on legal scholarship. Importing people rather than ideas from other disciplines threatens the role of legal scholarship as a disciplinary meeting ground. The risk is that substituting disciplinary scholars for legal scholars will substitute disciplinary scholarship for the interdisciplinary scholarship currently prevalent in law. One scenario by which that might occur is for Ph.D. hiring to become ubiquitous, for the disciplines to divide the fields of law among them, and for peer review to eliminate legal scholarship that fails to meet disciplinary requirements. My vision is one in which empiricism is distinguished from statistics, methodological sophistication is valued only as a means of discovery, and all legal scholars feel free to report empirical findings. Two changes are central to achieving that vision. The first--already implemented in some schools--is to provide empirical legal scholars with assistance from non-tenure-track faculty statisticians. Doing so will relieve the pressure on law faculties to acquire statistical expertise by hiring Ph.D.s in tenure-track positions. The second is to build a culture in the law schools that values empirical discovery and the advancement of knowledge over methodological sophistication.},
	number = {1},
	urldate = {2016-03-11},
	journal = {Tulane Law Review},
	author = {LoPucki, Lynn M.},
	year = {2015},
	keywords = {DOCTOR of philosophy degree, FACULTY, LAW -- Methodology, LAW -- Study \& teaching, LAW schools -- United States, LAW teachers -- Employment},
	pages = {1--34}
}

@article{george_empirical_2006,
	title = {An {Empirical} {Study} of {Empirical} {Legal} {Scholarship}: {The} {Top} {Law} {Schools}},
	volume = {81},
	shorttitle = {Empirical {Study} of {Empirical} {Legal} {Scholarship}},
	url = {http://heinonline.org/HOL/Page?handle=hein.journals/indana81&id=151&div=15&collection=journals},
	number = {1},
	journal = {Indiana Law Journal},
	author = {George, Tracey E.},
	year = {2006},
	pages = {141--161}
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1023/A%3A1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2016-03-07},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computing Methodologies, Language Translation and Linguistics, Simulation and Modeling, classification, ensemble, regression},
	pages = {5--32}
}

@article{berk_forecasting_2016,
	title = {Forecasting {Domestic} {Violence}: {A} {Machine} {Learning} {Approach} to {Help} {Inform} {Arraignment} {Decisions}},
	volume = {13},
	copyright = {© 2016, Copyright the Authors. Journal compilation © 2016, Cornell Law School and Wiley Periodicals, Inc.},
	issn = {1740-1461},
	shorttitle = {Forecasting {Domestic} {Violence}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/jels.12098/abstract},
	doi = {10.1111/jels.12098},
	abstract = {Arguably the most important decision at an arraignment is whether to release an offender until the date of his or her next scheduled court appearance. Under the Bail Reform Act of 1984, threats to public safety can be a key factor in that decision. Implicitly, a forecast of “future dangerousness” is required. In this article, we consider in particular whether usefully accurate forecasts of domestic violence can be obtained. We apply machine learning to data on over 28,000 arraignment cases from a major metropolitan area in which an offender faces domestic violence charges. One of three possible post-arraignment outcomes is forecasted within two years: (1) a domestic violence arrest associated with a physical injury, (2) a domestic violence arrest not associated with a physical injury, and (3) no arrests for domestic violence. We incorporate asymmetric costs for different kinds of forecasting errors so that very strong statistical evidence is required before an offender is forecasted to be a good risk. When an out-of-sample forecast of no post-arraignment domestic violence arrests within two years is made, it is correct about 90 percent of the time. Under current practice within the jurisdiction studied, approximately 20 percent of those released after an arraignment for domestic violence are arrested within two years for a new domestic violence offense. If magistrates used the methods we have developed and released only offenders forecasted not to be arrested for domestic violence within two years after an arraignment, as few as 10 percent might be arrested. The failure rate could be cut nearly in half. Over a typical 24-month period in the jurisdiction studied, well over 2,000 post-arraignment arrests for domestic violence perhaps could be averted.},
	language = {en},
	number = {1},
	urldate = {2016-03-07},
	journal = {Journal of Empirical Legal Studies},
	author = {Berk, Richard A. and Sorenson, Susan B. and Barnes, Geoffrey},
	month = mar,
	year = {2016},
	pages = {94--115}
}

@article{anzalone_zen_2015,
	title = {Zen and the {Art} of {Multitasking}: {Mindfulness} for {Law} {Librarians}},
	volume = {107},
	issn = {00239283},
	shorttitle = {Zen and the {Art} of {Multitasking}},
	url = {http://www.proxy.its.virginia.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=lxh&AN=112021625&site=eds-live},
	abstract = {Professor Anzalone explains what mindfulness is and how it can help law librarians thrive in their professional and personal lives. The demands on law librarians are tremendous, and the constant juggling of job responsibilities often leads to depression and burnout. A mindfulness practice can offer relief and perhaps even bring joy to harried law librarians.},
	number = {4},
	urldate = {2016-03-02},
	journal = {Law Library Journal},
	author = {Anzalone, Filippa Marullo},
	year = {2015},
	keywords = {BURNOUT (Psychology), HUMAN multitasking, JOB stress, LAW librarians, MINDFULNESS (Psychology)},
	pages = {561--577}
}

@incollection{burton_doing_2013,
	address = {New York},
	title = {Doing {Empirical} {Research}: {Exploring} the {Decision}-making of {Magistrates} and {Juries}},
	booktitle = {Research {Methods} in {Law}},
	publisher = {Routledge},
	author = {Burton, Mandy},
	editor = {Watkins, Dawn and Burton, Mandy},
	year = {2013},
	pages = {55--70}
}

@book{noauthor_research_2013,
	address = {London ; New York},
	edition = {1 edition},
	title = {Research {Methods} in {Law}},
	isbn = {978-0-415-67215-3},
	language = {English},
	publisher = {Routledge},
	month = sep,
	year = {2013}
}

@article{epstein_effective_2007,
	title = {On the {Effective} {Communication} of the {Results} of {Empirical} {Studies}, {Part} {II}},
	volume = {60},
	url = {http://heinonline.org/HOL/Page?handle=hein.journals/vanlr60&id=811&div=33&collection=journals},
	journal = {Vanderbilt Law Review},
	author = {Epstein, Lee and Martin, Andrew D. and Boyd, Christina L.},
	year = {2007},
	pages = {801--846}
}